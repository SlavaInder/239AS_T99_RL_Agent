{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "RL",
      "language": "python",
      "name": "rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Vanilla_Tetris_Agent_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ17OQYGrFRX"
      },
      "source": [
        "## RUN THIS IF YOU ARE RUNNING ON GOOGLE COLAB\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change Directory\n",
        "os.chdir('/content/drive/MyDrive/239AS_T99_RL_Agent/gym-t99')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2QlQc31rFRa"
      },
      "source": [
        "import sys\n",
        "\n",
        "# list directories where packages are stored\n",
        "# note that the parent directory of te repo is added automatically\n",
        "GYM_FOLDER = \"gym-t99\"\n",
        "\n",
        "# get this notebook's current working directory\n",
        "nb_cwd = os.getcwd()\n",
        "# get name of its parent directory\n",
        "nb_parent = os.path.dirname(nb_cwd)\n",
        "# add packages to path\n",
        "sys.path.insert(len(sys.path), nb_parent)\n",
        "sys.path.insert(len(sys.path), os.path.join(nb_parent, GYM_FOLDER))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH-fgoK8v-91"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq77xYdprFRb"
      },
      "source": [
        "import gym\n",
        "registered = gym.envs.registration.registry.env_specs.copy()\n",
        "\n",
        "import gym_t99\n",
        "import t_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMvjrmJKrFRc"
      },
      "source": [
        "import imp\n",
        "\n",
        "# this code removes environment from gym's registry\n",
        "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
        "for env in env_dict:\n",
        "    if env not in registered:\n",
        "        print(\"Remove {} from registry\".format(env))\n",
        "        del gym.envs.registration.registry.env_specs[env]\n",
        "\n",
        "imp.reload(gym_t99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLCv4Pf6rFRd"
      },
      "source": [
        "# import matplotlib\n",
        "import matplotlib as plt\n",
        "# configure matplotlib\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMzHDmH-rFRd"
      },
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-RLdSIYrFRd"
      },
      "source": [
        "# import torch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# configure torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFzwui2zrFRe"
      },
      "source": [
        "# a simple random agent to play aginst and test\n",
        "class RandomEnemySC:    \n",
        "    # this interface needs to be supported for any agent\n",
        "    def action(self, observation):\n",
        "        return np.random.choice(observation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd_lQNyzrFRe"
      },
      "source": [
        "custom_gym = gym.make('gym_t99:t99sc-v0', num_players = 1, enemy=RandomEnemySC())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfz_vUserFRe"
      },
      "source": [
        "# Example action and step\n",
        "\n",
        "action = {\n",
        "    \"reward\": 0,\n",
        "    \"state\": deepcopy(custom_gym.state)\n",
        "}\n",
        "\n",
        "observations, reward, done, _ = custom_gym.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfIXAfFRrFRf"
      },
      "source": [
        "def number_of_holes(board):\n",
        "    '''Number of holes in the board (empty sqquare with at least one block above it)'''\n",
        "    holes = 0\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "        # find the first non-empty cell in a column\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "        # find all the empty cells after the first non-empty cell.\n",
        "        # These count as holes.\n",
        "        holes += len([x for x in col[i+1:] if x == 0])\n",
        "\n",
        "    return holes\n",
        "\n",
        "\n",
        "def bumpiness(board):\n",
        "    '''Sum of the differences of heights between pair of columns'''\n",
        "    total_bumpiness = 0\n",
        "    max_bumpiness = 0\n",
        "    min_ys = []\n",
        "\n",
        "    # Find the first cell in each column that is non-zero. \n",
        "    # This is the height of the column.\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "        min_ys.append(i)\n",
        "\n",
        "    # Find the difference between consecutive heights.\n",
        "    # This is the bumpiness.\n",
        "    for i in range(len(min_ys) - 1):\n",
        "        bumpiness = abs(min_ys[i] - min_ys[i+1])\n",
        "        max_bumpiness = max(bumpiness, max_bumpiness)\n",
        "        total_bumpiness += abs(min_ys[i] - min_ys[i+1])\n",
        "\n",
        "    return total_bumpiness, max_bumpiness\n",
        "\n",
        "\n",
        "def height(board):\n",
        "    '''Sum and maximum height of the board'''\n",
        "    sum_height = 0\n",
        "    max_height = 0\n",
        "    min_height = board.shape[0]\n",
        "\n",
        "\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "\n",
        "        # Find the height of the first column\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "\n",
        "        # Update sum of heights, the max height, and the min height\n",
        "        height = board.shape[0] - i\n",
        "        sum_height += height\n",
        "        if height > max_height:\n",
        "            max_height = height\n",
        "        elif height < min_height:\n",
        "            min_height = height\n",
        "\n",
        "    return sum_height, max_height, min_height\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vutHNm0DrFRf"
      },
      "source": [
        "def calculate_features(player):\n",
        "  \"\"\"Creates a vector of features to represent a player's board.\n",
        "  \"\"\"\n",
        "  num_rows = player.board.shape[0]\n",
        "  num_cols = player.board.shape[1]\n",
        "  # Extract the board\n",
        "  board = player.board[5:num_rows-3, 3:num_cols-3]\n",
        "  # calculate lines cleared, holes, bumpiness, and heights\n",
        "  lines = player.num_lines_cleared\n",
        "  holes = number_of_holes(board)\n",
        "  total_bumpiness, max_bumpiness = bumpiness(board)\n",
        "  sum_height, max_height, min_height = height(board)\n",
        "  return np.array([lines, holes, total_bumpiness, sum_height])\n",
        "\n",
        "\n",
        "def calculate_next_state_features(next_states):\n",
        "  \"\"\"Calculates the list of features for all the next states.\n",
        "  \"\"\"\n",
        "  next_state_features = []\n",
        "  for state in next_states:\n",
        "      # use the function above to calculate features for current player\n",
        "      features = calculate_features(state.players[0])\n",
        "      next_state_features.append(features)\n",
        "  return next_state_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv6ETljmrFRg"
      },
      "source": [
        "\"\"\"\n",
        "@author: Viet Nguyen <nhviet1009@gmail.com>\n",
        "\"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "class DQN(nn.Module):\n",
        "  \"\"\"Class to create deep Q network\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "      super(DQN, self).__init__()\n",
        "\n",
        "      # Currently consists of 3 linear layers\n",
        "\n",
        "      self.layer1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n",
        "      self.layer2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
        "      self.layer3 = nn.Sequential(nn.Linear(64, 1))\n",
        "\n",
        "      self._create_weights()\n",
        "\n",
        "  def _create_weights(self):\n",
        "      for m in self.modules():\n",
        "          if isinstance(m, nn.Linear):\n",
        "              nn.init.xavier_uniform_(m.weight)\n",
        "              nn.init.constant_(m.bias, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Forward pass\n",
        "    \n",
        "    # move to appropriate device (cpu or gpu if present)\n",
        "      x = x.to(device)\n",
        "      x = self.layer1(x)\n",
        "      x = self.layer2(x)\n",
        "      x = self.layer3(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqWAe9rprFRg"
      },
      "source": [
        "from collections import namedtuple\n",
        "# Transition is same as experience.\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    \"\"\"A cyclic buffer i.e. newer experiences over-write \n",
        "    the older experiences when the capacity of ReplayMemory\n",
        "    is reached.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "    \n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1)%self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhV6BnMNrFRh"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from random import randint\n",
        "\n",
        "# Hyper parameters\n",
        "\n",
        "# BATCH_SIZE is the batch size used when training with experience replay\n",
        "BATCH_SIZE = 512\n",
        "# Gamma is the discount factor\n",
        "GAMMA = 0.95\n",
        "# starting value of epsilon i.e. the exploration parameter\n",
        "EPS_START = 1\n",
        "# ending value of epsilon\n",
        "EPS_END = 0\n",
        "# Number of episodes after which epsilon becomes 0.\n",
        "EPS_STOP_EPISODE = 1500\n",
        "# TARGET_UPDATE is how often we update the target network.\n",
        "TARGET_UPDATE = 100 \n",
        "\n",
        "\n",
        "# Initialize policy net to evaluate action and target net to pick action\n",
        "policy_net = DQN()\n",
        "target_net = DQN()\n",
        "# Load same initial parameters into policy_net\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "# Initialize ADAM optimizer and mean squared error loss function.\n",
        "optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# initialze data structure to hold experience replay\n",
        "memory = ReplayMemory(30000)\n",
        "\n",
        "# calculate the constant value by which we decrease epsilon after each iteration.\n",
        "eps_threshold = (EPS_START - EPS_END) / EPS_STOP_EPISODE\n",
        "\n",
        "# actual value of epsilon\n",
        "eps = EPS_START\n",
        "\n",
        "\n",
        "def select_action(next_states):\n",
        "    \"\"\"Follows Epsilon-Greedy selection strategy\n",
        "    \"\"\"\n",
        "    global steps_done\n",
        "    global eps_threshold\n",
        "    global eps\n",
        "    \n",
        "\n",
        "    # sample random number from 0 to 1\n",
        "    sample = random.random()\n",
        "    # result to be returned\n",
        "    steps_done += 1\n",
        "    \n",
        "\n",
        "    # create tensor of all next states\n",
        "    next_states = [torch.from_numpy(next_state) for next_state in next_states]\n",
        "    \n",
        "    # return index\n",
        "    index = None\n",
        "\n",
        "    # choose random action if <= epsilon\n",
        "    if sample <= eps:\n",
        "        index = randint(0, len(next_states) - 1)\n",
        "    else:\n",
        "      # otherwise choose greedy action\n",
        "\n",
        "        # create one big tensor out of all the next state features\n",
        "        next_states = torch.stack(next_states).type(torch.FloatTensor)\n",
        "        # obtain the Q values for each of the next states (which are really actions)\n",
        "        predictions = policy_net(next_states)[:, 0]\n",
        "        # obtain the index of the maximum Q value. this is the action that yields maximum expected return.\n",
        "        index = torch.argmax(predictions).item()\n",
        "    \n",
        "    # update the value of epsilon. This is constant update.\n",
        "    if eps > EPS_END:\n",
        "        eps -= eps_threshold\n",
        "    \n",
        "    # return the chosen index\n",
        "    return index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQdPZ7NrFRh"
      },
      "source": [
        "import math\n",
        "import random\n",
        "from itertools import count\n",
        "\n",
        "def optimize_model():\n",
        "\n",
        "    # If the number of experiences is less than the batch size, \n",
        "    # then we need to collect more experience samples before optimizing.\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "\n",
        "    # Sample a batch from memory\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    \n",
        "    # Create a batch of transitions\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    \n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    \n",
        "    # obtain the next states which are non-final (if final then its None)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    \n",
        "    # current state batch\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    # reward batch\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # obtain the state action values using policy net   \n",
        "    state_action_values = policy_net(state_batch)\n",
        "    \n",
        "    # calculate the state action values of the next states which are non-final using target net\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    \n",
        "    # obtain the expected return using the target net\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "\n",
        "    # calculate mse between the state action values generated by policy net and the expected return obtained from target net\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # optimize (calculate gradients using back prop and perform weight update)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HStFMkfFrFRh"
      },
      "source": [
        "time_lasted = []\n",
        "num_episodes = 3000\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    custom_gym.reset()\n",
        "\n",
        "    # set the current state\n",
        "    current_state = custom_gym.state\n",
        "\n",
        "    action = {\n",
        "        \"reward\": 0,\n",
        "        \"state\": deepcopy(current_state)\n",
        "    }\n",
        "\n",
        "    # take one step to obtain the next states (result of possible actions)\n",
        "    observations, reward, done, _ = custom_gym.step(action)\n",
        "    next_states, next_state_rewards = observations\n",
        "\n",
        "\n",
        "    for t in count():\n",
        "        # obtain feature vectors\n",
        "        next_state_features = calculate_next_state_features(next_states)\n",
        "        # obtain the action to take using epsilon greedy action selection strategy.\n",
        "        next_state_index = select_action(next_state_features)\n",
        "        \n",
        "        # construct the action\n",
        "        action = {\n",
        "            \"reward\" : next_state_rewards[next_state_index],\n",
        "            \"state\" : next_states[next_state_index]\n",
        "        }\n",
        "        \n",
        "        # take a step to obtain reward and next possible states\n",
        "        new_next_states, reward, done, _ = custom_gym.step(action)\n",
        "        reward = torch.tensor([reward], device=device).type(torch.FloatTensor)\n",
        "        \n",
        "\n",
        "        # get the features of the next state\n",
        "        if not done:\n",
        "            save_next_state = torch.unsqueeze(torch.tensor(calculate_features(next_states[next_state_index].players[0])).type(torch.FloatTensor), 0) \n",
        "        else:\n",
        "            save_next_state = None\n",
        "            \n",
        "\n",
        "        # store experience for later optimization\n",
        "        memory.push(\n",
        "            torch.unsqueeze(torch.tensor(calculate_features(current_state.players[0])).type(torch.FloatTensor), 0), \n",
        "            action, \n",
        "            save_next_state, \n",
        "            reward\n",
        "        )\n",
        "\n",
        "\n",
        "        # update the current state based on the action chosen above\n",
        "        current_state = next_states[next_state_index]\n",
        "        # also update the set of next possible states (possible actions)\n",
        "        next_states, next_state_rewards = new_next_states\n",
        "\n",
        "        # optimize the model\n",
        "        optimize_model()\n",
        "        \n",
        "        # check if agent lost\n",
        "        if done:\n",
        "            # record the amount of time the agent lasted in this episode.\n",
        "            time_lasted.append(t)\n",
        "\n",
        "            # print the board at regular intervals to visually examine if agent is learning\n",
        "            if i_episode % 100 == 0 and i_episode != 0:\n",
        "                frame = custom_gym.render(mode=\"debug\")\n",
        "                print(frame[0])\n",
        "            break\n",
        "\n",
        "\n",
        "    # update the target net at the right interval with weights from the policy net.\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "\n",
        "print('Complete')\n",
        "custom_gym.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pe99L-hrFRi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "avgs = []\n",
        "num_avgs = []\n",
        "for i in range(0, len(time_lasted), 50):\n",
        "  avgs.append(np.average(time_lasted[i:i+50]))\n",
        "  num_avgs.append(i)\n",
        "plt.plot(num_avgs, avgs)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"time lasted\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}