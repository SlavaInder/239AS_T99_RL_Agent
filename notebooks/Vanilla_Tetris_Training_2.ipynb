{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "RL",
      "language": "python",
      "name": "rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Vanilla_Tetris_Training_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7089vFH5qHkU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Change Directory\n",
        "os.chdir('/content/drive/MyDrive/239AS_T99_RL_Agent/gym-t99')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_9t7dQsqGzi"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# list directories where packages are stored\n",
        "# note that the parent directory of te repo is added automatically\n",
        "GYM_FOLDER = \"gym-t99\"\n",
        "\n",
        "# get this notebook's current working directory\n",
        "nb_cwd = os.getcwd()\n",
        "# get name of its parent directory\n",
        "nb_parent = os.path.dirname(nb_cwd)\n",
        "# add packages to path\n",
        "sys.path.insert(len(sys.path), nb_parent)\n",
        "sys.path.insert(len(sys.path), os.path.join(nb_parent, GYM_FOLDER))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9k5sXY-qVdl"
      },
      "source": [
        "!pip install pygame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_b-CVWfqGzm"
      },
      "source": [
        "import gym\n",
        "registered = gym.envs.registration.registry.env_specs.copy()\n",
        "\n",
        "import gym_t99\n",
        "import t_net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr2cGUUXqGzn"
      },
      "source": [
        "import imp\n",
        "\n",
        "# this code removes environment from gym's registry\n",
        "env_dict = gym.envs.registration.registry.env_specs.copy()\n",
        "for env in env_dict:\n",
        "    if env not in registered:\n",
        "        print(\"Remove {} from registry\".format(env))\n",
        "        del gym.envs.registration.registry.env_specs[env]\n",
        "\n",
        "imp.reload(gym_t99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5XK4QjEqGzo"
      },
      "source": [
        "# import matplotlib\n",
        "import matplotlib as plt\n",
        "# configure matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "# import torch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# configure torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6ya-rdEqGzo"
      },
      "source": [
        "# a simple random agent to play aginst and test\n",
        "class RandomEnemySC:    \n",
        "    # this interface needs to be supported for any agent\n",
        "    def action(self, observation):\n",
        "        return np.random.choice(observation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDEi5-Y7qGzo"
      },
      "source": [
        "custom_gym = gym.make('gym_t99:t99sc-v0', num_players = 1, enemy=RandomEnemySC())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZFWnFkbqGzp"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(nn.Linear(4, 64), nn.ReLU(inplace=True))\n",
        "        self.layer2 = nn.Sequential(nn.Linear(64, 64), nn.ReLU(inplace=True))\n",
        "        self.layer3 = nn.Sequential(nn.Linear(64, 1))\n",
        "\n",
        "        self._create_weights()\n",
        "\n",
        "    def _create_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0BGCUuxqGzp"
      },
      "source": [
        "def number_of_holes(board):\n",
        "    '''Number of holes in the board (empty sqquare with at least one block above it)'''\n",
        "    holes = 0\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "        # find the first non-empty cell in a column\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "        # find all the empty cells after the first non-empty cell.\n",
        "        # These count as holes.\n",
        "        holes += len([x for x in col[i+1:] if x == 0])\n",
        "\n",
        "    return holes\n",
        "\n",
        "\n",
        "def bumpiness(board):\n",
        "    '''Sum of the differences of heights between pair of columns'''\n",
        "    total_bumpiness = 0\n",
        "    max_bumpiness = 0\n",
        "    min_ys = []\n",
        "\n",
        "    # Find the first cell in each column that is non-zero. \n",
        "    # This is the height of the column.\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "        min_ys.append(i)\n",
        "\n",
        "    # Find the difference between consecutive heights.\n",
        "    # This is the bumpiness.\n",
        "    for i in range(len(min_ys) - 1):\n",
        "        bumpiness = abs(min_ys[i] - min_ys[i+1])\n",
        "        max_bumpiness = max(bumpiness, max_bumpiness)\n",
        "        total_bumpiness += abs(min_ys[i] - min_ys[i+1])\n",
        "\n",
        "    return total_bumpiness, max_bumpiness\n",
        "\n",
        "\n",
        "def height(board):\n",
        "    '''Sum and maximum height of the board'''\n",
        "    sum_height = 0\n",
        "    max_height = 0\n",
        "    min_height = board.shape[0]\n",
        "\n",
        "\n",
        "    for col in zip(*board):\n",
        "        i = 0\n",
        "\n",
        "        # Find the height of the first column\n",
        "        while i < board.shape[0] and col[i] == 0:\n",
        "            i += 1\n",
        "\n",
        "        # Update sum of heights, the max height, and the min height\n",
        "        height = board.shape[0] - i\n",
        "        sum_height += height\n",
        "        if height > max_height:\n",
        "            max_height = height\n",
        "        elif height < min_height:\n",
        "            min_height = height\n",
        "\n",
        "    return sum_height, max_height, min_height\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc-8qvPIqGzq"
      },
      "source": [
        "def calculate_features(player):\n",
        "  \"\"\"Creates a vector of features to represent a player's board.\n",
        "  \"\"\"\n",
        "  num_rows = player.board.shape[0]\n",
        "  num_cols = player.board.shape[1]\n",
        "  # Extract the board\n",
        "  board = player.board[5:num_rows-3, 3:num_cols-3]\n",
        "  # calculate lines cleared, holes, bumpiness, and heights\n",
        "  lines = player.num_lines_cleared\n",
        "  holes = number_of_holes(board)\n",
        "  total_bumpiness, max_bumpiness = bumpiness(board)\n",
        "  sum_height, max_height, min_height = height(board)\n",
        "  return torch.tensor([lines, holes, total_bumpiness, sum_height])\n",
        "\n",
        "\n",
        "def calculate_next_state_features(next_states):\n",
        "  \"\"\"Calculates the list of features for all the next states.\n",
        "  \"\"\"\n",
        "  next_state_features = []\n",
        "  for state in next_states:\n",
        "      # use the function above to calculate features for current player\n",
        "      features = calculate_features(state.players[0])\n",
        "      next_state_features.append(features)\n",
        "  return next_state_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmV-e_ynqGzq"
      },
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "import random\n",
        "from random import randint\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "\n",
        "    '''Deep Q Learning Agent + Maximin\n",
        "    Args:\n",
        "        state_size (int): Size of the input domain\n",
        "        mem_size (int): Size of the replay buffer\n",
        "        discount (float): How important is the future rewards compared to the immediate ones [0,1]\n",
        "        epsilon (float): Exploration (probability of random values given) value at the start\n",
        "        epsilon_min (float): At what epsilon value the agent stops decrementing it\n",
        "        epsilon_stop_episode (int): At what episode the agent stops decreasing the exploration variable\n",
        "        n_neurons (list(int)): List with the number of neurons in each inner layer\n",
        "        activations (list): List with the activations used in each inner layer, as well as the output\n",
        "        loss (obj): Loss function\n",
        "        optimizer (obj): Otimizer used\n",
        "        replay_start_size: Minimum size needed to train\n",
        "    '''\n",
        "\n",
        "    def __init__(self, state_size, mem_size=10000, discount=0.95,\n",
        "                 epsilon=1, epsilon_min=0, epsilon_stop_episode=500,\n",
        "                 n_neurons=[32,32], activations=['relu', 'relu', 'linear'],\n",
        "                 loss='mse', optimizer='adam', replay_start_size=None):\n",
        "\n",
        "        assert len(activations) == len(n_neurons) + 1\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.memory = deque(maxlen=mem_size)\n",
        "        self.discount = discount\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / (epsilon_stop_episode)\n",
        "        self.n_neurons = n_neurons\n",
        "        self.activations = activations\n",
        "        if not replay_start_size:\n",
        "            replay_start_size = mem_size / 2\n",
        "        self.replay_start_size = replay_start_size\n",
        "        self.policy_net, self.optimizer, self.criterion = self._build_model()\n",
        "\n",
        "\n",
        "    def _build_model(self):\n",
        "        '''Builds a Keras deep neural network model'''\n",
        "        policy_net = DQN().to(device)\n",
        "        optimizer = torch.optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "        criterion = nn.MSELoss()\n",
        "        \n",
        "        return policy_net, optimizer, criterion\n",
        "\n",
        "\n",
        "    def add_to_memory(self, current_state, next_state, reward, done):\n",
        "        '''Adds a play to the replay memory buffer'''\n",
        "        self.memory.append((current_state, next_state, reward, done))\n",
        "\n",
        "\n",
        "    def random_value(self):\n",
        "        '''Random score for a certain action'''\n",
        "        return random.random()\n",
        "\n",
        "\n",
        "    def predict_value(self, state):\n",
        "        '''Predicts the score for a certain state'''\n",
        "        return self.model(state)[0]\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        '''Returns the expected score of a certain state'''\n",
        "        state = np.reshape(state, [1, self.state_size])\n",
        "        if random.random() <= self.epsilon:\n",
        "            return self.random_value()\n",
        "        else:\n",
        "            return self.predict_value(state)\n",
        "\n",
        "\n",
        "    def best_state(self, states):\n",
        "        '''Returns the best state for a given collection of states'''\n",
        "        max_value = None\n",
        "        best_state = None\n",
        "\n",
        "        if random.random() <= self.epsilon:\n",
        "            return randint(0, len(states) - 1)\n",
        "\n",
        "        else:\n",
        "            next_states = torch.stack(states).type(torch.FloatTensor)\n",
        "            predictions = self.policy_net(next_states)[:, 0]\n",
        "            index = torch.argmax(predictions).item()\n",
        "            return index\n",
        "\n",
        "\n",
        "    def train(self, batch_size=32, epochs=3):\n",
        "        '''Trains the agent'''\n",
        "        n = len(self.memory)\n",
        "\n",
        "        if n >= self.replay_start_size and n >= batch_size:\n",
        "\n",
        "            batch = random.sample(self.memory, batch_size)\n",
        "\n",
        "            # Get the expected score for the next states\n",
        "\n",
        "            next_states = [x[2] for x in batch]\n",
        "\n",
        "            next_qs = []\n",
        "            for state in next_states:\n",
        "                if state != None:\n",
        "                    next_qs.append(self.policy_net(state))\n",
        "                else:\n",
        "                    next_qs.append(0)\n",
        "\n",
        "            x = []\n",
        "            y = []\n",
        "\n",
        "            # Build xy structure to fit the model in a batch\n",
        "            for i, (state, reward, _, done) in enumerate(batch):\n",
        "                new_q = None\n",
        "                if not done:\n",
        "                    # Partial Q formula\n",
        "                    new_q = reward + self.discount * next_qs[i]\n",
        "\n",
        "                else:\n",
        "                    new_q = torch.unsqueeze(reward, 0)\n",
        "\n",
        "                x.append(self.policy_net(state))\n",
        "                y.append(new_q)\n",
        "            \n",
        "            x = torch.cat(x)\n",
        "            y = torch.cat(y)\n",
        "\n",
        "            # Fit the model to the given values\n",
        "            loss = self.criterion(x, y)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Update the exploration variable\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon -= self.epsilon_decay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-993y5ZqGzu"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# hyperparameters\n",
        "\n",
        "# Total number of episodes\n",
        "episodes = 4000\n",
        "\n",
        "# Maximum number of time steps for an episode. If number of timesteps exceeds this number the episode terminates.\n",
        "max_steps = None\n",
        "\n",
        "# Episode at which epsilon decreases to 0.\n",
        "epsilon_stop_episode = 3000\n",
        "\n",
        "# The maximum number of experiences that can be collected\n",
        "mem_size = 20000\n",
        "\n",
        "# Discount factor\n",
        "discount = 0.95\n",
        "\n",
        "# Size of experience batch to train DQN\n",
        "batch_size = 512\n",
        "\n",
        "# minimum number experiences needed before training starts again.\n",
        "replay_start_size = 2000\n",
        "\n",
        "train_every = 1\n",
        "epochs=1\n",
        "\n",
        "\n",
        "# initialize agent\n",
        "agent = DQNAgent(4,\n",
        "                 epsilon_stop_episode=epsilon_stop_episode, mem_size=mem_size,\n",
        "                 discount=discount, replay_start_size=replay_start_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# list to record the total timesteps the agent lasted in each episode.\n",
        "time_lasted = []\n",
        "\n",
        "for episode in range(episodes):\n",
        "    # Initialize the environment and state\n",
        "    custom_gym.reset()\n",
        "\n",
        "    # set the current state\n",
        "    current_state = custom_gym.state\n",
        "\n",
        "\n",
        "    action = {\n",
        "        \"reward\": 0,\n",
        "        \"state\": deepcopy(custom_gym.state)\n",
        "    }\n",
        "\n",
        "\n",
        "    # take one step to obtain the next states (result of possible actions)\n",
        "    observations, reward, done, _ = custom_gym.step(action)\n",
        "    next_states, next_state_rewards = observations\n",
        "\n",
        "    # init done and num steps for the current episode.\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    # Game\n",
        "    while not done and (not max_steps or steps < max_steps):\n",
        "        # obtain feature vectors\n",
        "        next_states_features = calculate_next_state_features(next_states)\n",
        "        # obtain the action to take using epsilon greedy action selection strategy.\n",
        "        best_state_index = agent.best_state(next_states_features)\n",
        "        \n",
        "        # construct the action\n",
        "        action = {\n",
        "            \"reward\" : next_state_rewards[best_state_index],\n",
        "            \"state\" : next_states[best_state_index]\n",
        "        }\n",
        "        \n",
        "        # take a step to obtain reward and next possible states\n",
        "        new_next_states, reward, done, _ = custom_gym.step(action)\n",
        "        reward = torch.tensor([reward], device=device).type(torch.FloatTensor).to(device)\n",
        "        \n",
        "        # get the features of the next state\n",
        "        if not done:\n",
        "            save_next_state = torch.unsqueeze(torch.tensor(calculate_features(next_states[best_state_index].players[0])).type(torch.FloatTensor), 0) \n",
        "        else:\n",
        "            save_next_state = None\n",
        "        \n",
        "        # store experience for later optimization\n",
        "        agent.add_to_memory(\n",
        "            torch.unsqueeze(torch.tensor(calculate_features(current_state.players[0])).type(torch.FloatTensor), 0), \n",
        "            reward,\n",
        "            save_next_state,\n",
        "            done\n",
        "        )\n",
        "        \n",
        "        # update the current state based on the action chosen above\n",
        "        current_state = next_states[best_state_index]\n",
        "        # also update the set of next possible states (possible actions)\n",
        "        next_states, next_state_rewards = new_next_states\n",
        "        steps += 1\n",
        "\n",
        "    # record the number of steps the current episode lasted.\n",
        "    time_lasted.append(steps)\n",
        "\n",
        "    # Train after playing an episode\n",
        "    if episode % train_every == 0:\n",
        "        agent.train(batch_size=batch_size, epochs=epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnr6HfwBqGzv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "xs = [i for i in range(1, 3964)]\n",
        "plt.plot(xs, time_lasted)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"time lasted\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzpIjkEwqGzv"
      },
      "source": [
        "PATH=\"./model_with_swap_1\"\n",
        "torch.save(agent.policy_net.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}